// @title DKonsole API
// @version 1.5.2
// @description API para gestiÃ³n de recursos Kubernetes y Helm releases
// @termsOfService http://swagger.io/terms/

// @contact.name DKonsole Support
// @contact.url https://github.com/flaucha/DKonsole/backend

// @license.name Apache 2.0
// @license.url http://www.apache.org/licenses/LICENSE-2.0.html

// @host localhost:8080
// @BasePath /
// @schemes http https

// @securityDefinitions.apikey Bearer
// @in header
// @name Authorization
// @description JWT token obtenido del endpoint /api/login. Formato: "Bearer {token}"

package main

import (
	"context"
	"flag"
	"net/http"
	"os"
	"path/filepath"
	"strings"
	"time"

	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/client-go/util/homedir"
	metricsv "k8s.io/metrics/pkg/client/clientset/versioned"

	"github.com/flaucha/DKonsole/backend/internal/api"
	"github.com/flaucha/DKonsole/backend/internal/auth"
	"github.com/flaucha/DKonsole/backend/internal/cluster"
	"github.com/flaucha/DKonsole/backend/internal/helm"
	"github.com/flaucha/DKonsole/backend/internal/k8s"
	"github.com/flaucha/DKonsole/backend/internal/ldap"
	"github.com/flaucha/DKonsole/backend/internal/logo"
	"github.com/flaucha/DKonsole/backend/internal/models"
	"github.com/flaucha/DKonsole/backend/internal/pod"
	"github.com/flaucha/DKonsole/backend/internal/prometheus"
	"github.com/flaucha/DKonsole/backend/internal/server"
	"github.com/flaucha/DKonsole/backend/internal/settings"
	"github.com/flaucha/DKonsole/backend/internal/utils"

	_ "github.com/flaucha/DKonsole/backend/docs" // docs is generated by Swag CLI
)

func main() {
	var kubeconfig *string
	if home := homedir.HomeDir(); home != "" {
		kubeconfig = flag.String("kubeconfig", filepath.Join(home, ".kube", "config"), "(optional) absolute path to the kubeconfig file")
	} else {
		kubeconfig = flag.String("kubeconfig", "", "absolute path to the kubeconfig file")
	}
	flag.Parse()

	// Initial config build
	config, err := buildKubeConfig(*kubeconfig, "")
	if err != nil {
		// Log warning but proceed (might be setup mode)
		utils.LogWarn("Failed to build initial kubeconfig (Setup Mode likely required)", map[string]interface{}{
			"error": err.Error(),
		})
	}

	// create the clientset
	var clientset *kubernetes.Clientset
	var metricsClient *metricsv.Clientset
	var dynamicClient *dynamic.DynamicClient

	if config != nil {
		clientset, err = kubernetes.NewForConfig(config)
		if err != nil {
			utils.LogError(err, "Failed to create clientset", nil)
			os.Exit(1)
		}

		metricsClient, err = metricsv.NewForConfig(config)
		if err != nil {
			utils.LogWarn("Failed to create metrics client; metrics endpoints will be disabled", map[string]interface{}{
				"error": err.Error(),
			})
		}

		dynamicClient, err = dynamic.NewForConfig(config)
		if err != nil {
			utils.LogError(err, "Failed to create dynamic client", nil)
			os.Exit(1)
		}
	} else {
		utils.LogInfo("Starting in Setup Mode (no K8s client available)", nil)
	}

	// Initialize auth service with Kubernetes client
	// Secret name follows Helm chart convention: {release-name}-auth, default: dkonsole-auth
	const defaultSecretName = "dkonsole-auth" // #nosec G101 -- This is a secret name, not a credential
	secretName := os.Getenv("AUTH_SECRET_NAME")
	if secretName == "" {
		secretName = defaultSecretName
	}

	// Try to get Prometheus URL from ConfigMap first, then fallback to environment variable
	prometheusURL := os.Getenv("PROMETHEUS_URL")
	if clientset != nil {
		// Try to read from ConfigMap
		settingsRepo := settings.NewRepository(clientset, secretName)
		if url, err := settingsRepo.GetPrometheusURL(context.Background()); err == nil && url != "" {
			prometheusURL = url
		}
	}

	handlersModel := &models.Handlers{
		Clients:       make(map[string]kubernetes.Interface),
		Dynamics:      make(map[string]dynamic.Interface),
		Metrics:       make(map[string]*metricsv.Clientset),
		RESTConfigs:   make(map[string]*rest.Config),
		PrometheusURL: prometheusURL,
	}
	handlersModel.Clients["default"] = clientset
	handlersModel.Dynamics["default"] = dynamicClient
	handlersModel.RESTConfigs["default"] = config
	if metricsClient != nil {
		handlersModel.Metrics["default"] = metricsClient
	}

	authService, err := auth.NewService(clientset, secretName)
	if err != nil {
		utils.LogError(err, "Failed to initialize auth service", nil)
		os.Exit(1)
	}

	// Register callback to update global clients on reload
	authService.OnReload = func(token string) {
		utils.LogInfo("Reloading global K8s clients with new token", nil)
		newConfig, err := buildKubeConfig(*kubeconfig, token)
		if err != nil {
			utils.LogError(err, "Failed to build new config during reload", nil)
			return
		}

		newClientset, err := kubernetes.NewForConfig(newConfig)
		if err != nil {
			utils.LogError(err, "Failed to create new clientset during reload", nil)
			return
		}

		newMetricsClient, err := metricsv.NewForConfig(newConfig)
		if err != nil {
			utils.LogWarn("Failed to create new metrics client during reload", map[string]interface{}{
				"error": err.Error(),
			})
		}

		newDynamicClient, err := dynamic.NewForConfig(newConfig)
		if err != nil {
			utils.LogError(err, "Failed to create new dynamic client during reload", nil)
			return
		}

		// Update global handlers model with thread safety
		handlersModel.Lock()
		handlersModel.Clients["default"] = newClientset
		handlersModel.Dynamics["default"] = newDynamicClient
		handlersModel.RESTConfigs["default"] = newConfig
		if newMetricsClient != nil {
			handlersModel.Metrics["default"] = newMetricsClient
		}
		handlersModel.Unlock()

		utils.LogInfo("Global K8s clients updated successfully", nil)
	}

	// Initialize LDAP service (non-blocking - will work even if LDAP is not configured)
	ldapFactory := ldap.NewServiceFactory(clientset, secretName)
	ldapService := ldapFactory.NewService()

	// Connect LDAP to auth service (non-blocking - will check if enabled on first login)
	// We set it up here so it's available, but it won't block startup if LDAP is not configured
	authService.SetLDAPAuthenticator(ldapService)

	clusterService := cluster.NewService(handlersModel)
	k8sService := k8s.NewService(handlersModel, clusterService)
	apiService := api.NewService(clusterService)
	helmService := helm.NewService(handlersModel, clusterService)
	podService := pod.NewService(handlersModel, clusterService)
	prometheusService := prometheus.NewHTTPHandler(handlersModel.PrometheusURL, clusterService)

	// Get namespace for logo ConfigMap (default to "dkonsole")
	logoNamespace := os.Getenv("POD_NAMESPACE")
	if logoNamespace == "" {
		// Try to read from service account namespace file
		if nsBytes, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/namespace"); err == nil {
			logoNamespace = strings.TrimSpace(string(nsBytes))
		}
	}
	if logoNamespace == "" {
		logoNamespace = "dkonsole" // Default fallback
	}

	logoService := logo.NewService(clientset, logoNamespace)
	settingsFactory := settings.NewServiceFactory(clientset, handlersModel, secretName, prometheusService)
	settingsService := settingsFactory.NewService()

	router := server.NewRouter(server.Dependencies{
		AuthService:       authService,
		LDAPService:       ldapService,
		ClusterService:    clusterService,
		K8sService:        k8sService,
		APIService:        apiService,
		HelmService:       helmService,
		PodService:        podService,
		PrometheusService: prometheusService,
		SettingsService:   settingsService,
		LogoService:       logoService,
		HandlersModel:     handlersModel,
		StaticDir:         "static",
	})

	port := ":8080"
	utils.LogInfo("Server starting", map[string]interface{}{
		"port": port,
	})
	// Configure HTTP server with timeouts
	srv := &http.Server{
		Addr:              port,
		Handler:           router,
		ReadHeaderTimeout: 5 * time.Second,
		ReadTimeout:       15 * time.Second,
		WriteTimeout:      15 * time.Second,
		IdleTimeout:       60 * time.Second,
	}

	utils.LogInfo("Server starting", map[string]interface{}{
		"port": port,
	})
	if err := srv.ListenAndServe(); err != nil {
		utils.LogError(err, "Server failed", nil)
		os.Exit(1)
	}
}

// buildKubeConfig constructs a kubernetes rest.Config based on flags, environment, or override token.
func buildKubeConfig(kubeconfigPath, overrideToken string) (*rest.Config, error) {
	// 1. If override token is provided, stick to InCluster-like manual config
	if overrideToken != "" {
		host := "https://" + os.Getenv("KUBERNETES_SERVICE_HOST") + ":" + os.Getenv("KUBERNETES_SERVICE_PORT")

		config := &rest.Config{
			Host:            host,
			BearerToken:     overrideToken,
			TLSClientConfig: rest.TLSClientConfig{Insecure: false},
		}
		// Try to read CA cert
		if caData, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"); err == nil {
			config.TLSClientConfig.CAData = caData
		} else {
			utils.LogWarn("Could not load CA certificate, connection might fail if self-signed", map[string]interface{}{
				"error": err.Error(),
			})
		}
		return config, nil
	}

	// 2. Try kubeconfig from flags
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfigPath)
	if err == nil {
		return config, nil
	}

	// 3. Try to read token from volume mount (Secret)
	tokenPath := "/etc/dkonsole/auth/service-account-token" //nolint:gosec // Path to file not credential
	tokenData, readErr := os.ReadFile(tokenPath)
	if readErr == nil && len(tokenData) > 0 {
		token := string(tokenData)
		config := &rest.Config{
			Host:            "https://" + os.Getenv("KUBERNETES_SERVICE_HOST") + ":" + os.Getenv("KUBERNETES_SERVICE_PORT"),
			BearerToken:     token,
			TLSClientConfig: rest.TLSClientConfig{Insecure: false},
		}
		if caData, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"); err == nil {
			config.TLSClientConfig.CAData = caData
		} else {
			utils.LogWarn("Could not load CA certificate for token auth, connection might fail if self-signed", map[string]interface{}{
				"error": err.Error(),
			})
		}
		utils.LogInfo("Authenticated using service account token from volume", nil)
		return config, nil
	}

	// 4. Fallback to InClusterConfig
	// Note: InClusterConfig fails if not in cluster.
	return rest.InClusterConfig()
}
